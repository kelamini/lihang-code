# K 近邻法

## 算法 3.1（k 近邻法）

**输入**：训练数据集

$$
T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}
$$

其中，$x_i \in \mathcal{X} \subseteq \mathbf{R}^n$ 为实例的特征向量，$y_i \in \mathcal{Y} = \{c_1, c_2, ..., c_K\}$ 为实例的类别， $i=1, 2, ..., N$；实例特征向量 $x$；

**输出**：实例 $x$ 所属的类 $y$。
（1）根据给定的距离度量，在训练数据 $T$ 中找出与 $x$ 最近邻的 $k$ 个点，涵盖这 $k$ 个点的 $x$ 的邻域记作 $N_k(x)$；
（2）在 $N_k(x)$ 中根据分类决策规则（如多数表决）决定 $x$ 的类别 $y$：

$$
y = arg \max_{c_j} \sum_{x_i \in N_k(x)}I(y_i=c_j),\ i = 1, 2, ..., N;\ j=1, 2, ..., K
$$

其中，$I$ 为指示函数，即当 $y_i=c_j$ 时 $I$ 为 $1$，否则 $I$ 为 $0$。

### 例 3.1

已知二维空间的 3 个点 $x_1=(1, 1)^T$， $x_2=(5, 1)^T$， $x_3=(4, 4)^T$，试求在 $p$ 取不同值时，$L_p$ 距离下 $x_1$ 的最近邻点。

解 因为 $x_1$ 和 $x_2$ 只有第一维的值不同，所以 $p$ 为任何值时， $L_p(x_1, x_2)=4$。而 $L_1(x_1, x_3)=6$， $L_2(x_1, x_3)=4.24$， $L_3(x_1, x_3)=3.78$， $L_4(x_1, x_3)=3.57$。于是得到： $p$ 等于 $1$ 或者 2 时， $x_2$ 是 $x_1$ 的最近邻点； $p$ 大于等于 3 时，$x_3$ 是 $x_1$ 的最近邻点。

## 算法 3.2 （构造平衡 kd 树）

**输入**：$k$ 维空间数据集 $T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$，其中 $x_i = (x_i^{(1)}, x_i^{(2)}, ..., x_i^{(k)})$，$i = 1, 2, ..., N$；
**输出**：kd 树。
（1）开始：构造根节点，根据点对应于包含 $T$ 的 $k$ 维空间的超矩形区域。

选择 $x^{(1)}$ 为坐标轴，以 $T$ 中所有实例点的 $x^{(1)}$ 坐标的中位数为切分点，将根节点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴 $x^{(1)}$ 垂直的超平面实现。

由根节点生成深度为 1 的左、右子结点：左子结点对应坐标 $x^{(1)}$ 小于切分点的子区域，右子结点对应于坐标 $x^{(1)}$ 大于切分点的子区域。

（2）重复：对深度为 $j$ 的结点，选择 $x^{(l)}$ 为切分的坐标轴，$l = j(mod \ k) + 1$，以该结点的区域中实例的 $x^{(l)}$ 坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴 $x^{(l)}$ 垂直的超平面实现。

由该节点生成深度为 $j + 1$ 的左、右子结点：左子结点对应坐标 $x^{(l)}$ 小于切分点的子区域，右子结点对应于坐标 $x^{(l)}$ 大于切分点的子区域。

将落在切分超平面的实例点保存在该节点。

（3）直到两个子区域没有实例存在时停止。从而行成 $kd$ 树的区域划分。

### 例 3.2

给定一个二维空间的数据集：$T = \{(2, 3)^T, (5, 4)^T, (9, 6)^T, (4, 7)^T, (8, 1)^T, (7, 2)^T\}$，构造 一个平衡 kd 树。

**解** 根结点对应包含

## 算法 3.3（用 kd 树的最近邻搜索）

输入：
输出：
（1）
（2）
（3）
（4）

### 例 3.3
