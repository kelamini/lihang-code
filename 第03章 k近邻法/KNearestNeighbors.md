# K 近邻法

## 算法 3.1（k 近邻法）

**输入**：训练数据集

$$T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$$ 

其中，$x_i \in \mathcal{X} \subseteq \mathbf{R}^n$ 为实例的特征向量，$y_i \in \mathcal{Y} = \{c_1, c_2, ..., c_K\}$ 为实例的类别，$i=1, 2, ..., N$；实例特征向量 $x$；

**输出**：实例 $x$ 所属的类 $y$。
**（1）**根据给定的距离度量，在训练数据 $T$ 中找出与 $x$ 最近邻的 $k$ 个点，涵盖这 $k$ 个点的 $x$ 的邻域记作 $N_k(x)$；
**（2）**在 $N_k(x)$ 中根据分类决策规则（如多数表决）决定 $x$ 的类别 $y$：

$$y = arg \max_{c_j} \sum_{x_i \in N_k(x)}I(y_i=c_j)$$，$$i = 1, 2, ..., N;\ j=1, 2, ..., K$$ 

其中，$I$ 为指示函数，即当 $y_i=c_j$ 时 $I$ 为 $1$，否则 $I$ 为 $0$。

### 例 3.1

已知二维空间的 3 个点 $x_1=(1, 1)^T$，$x_2=(5, 1)^T$，$x_3=(4, 4)^T$，试求在 $p$ 取不同值时，$L_p$ 距离下 $x_1$ 的最近邻点。

解 因为 $x_1$ 和 $x_2$ 只有第一维的值不同，所以 $p$ 为任何值时，$L_p(x_1, x_2)=4$。而 $L_1(x_1, x_3)=6$，$L_2(x_1, x_3)=4.24$，$L_3(x_1, x_3)=3.78$，$L_4(x_1, x_3)=3.57$。于是得到：$p$ 等于 $1$ 或者 2 时，$x_2$ 是 $x_1$ 的最近邻点；$p$ 大于等于 3 时，$x_3$ 是 $x_1$ 的最近邻点。

## 算法 3.2 （构造平衡 kd 树）

**输入**：$k$ 维空间数据集 $T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$，其中 $x_i = (x_i^{(1)}, x_i^{(2)}, ..., x_i^{(k)})$，$i = 1, 2, ..., N$；
**输出**：kd 树。
（1）
（2）
（3）