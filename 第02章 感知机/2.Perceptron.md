# 感知机

## 算法 2.1（感知机学习算法的原始形式）

**输入**：训练数据集 $T=\{(x_1,\ y_1), (x_1,\ y_1),\ ...,\ (x_N,\ y_N)\}$，其中 $x_i\ \in \mathcal{X}=\mathbf{R}^n,\ y_i \in \mathcal{Y}=\{-1,\ +1\},\ i=1,\ 2,\ ...,\ N$；学习率 $\eta\ (0\ <\ \eta\leqslant\ 1)$；
**输出**：$w, b$；感知机模型 $f(x)=sign(w\cdot\ x+b)$。
（1）选取初值 $w_0,\ b_0$；
（2）在训练集中选取数据 $(x_i,\ y_i)$；
（3）如果 $y_i(w\cdot\ x_i+b)\leqslant\ 0$，更新 $w,\ b$；

$$
w \leftarrow w + \eta y_i x_i
$$

$$
b \leftarrow b + \eta y_i
$$

（4）转至（2），直至训练集中没有误分类点。

### Python 实现

```python

```

### 例 2.1

如图所示的训练数据集，其正实例点是 $x_1=(3, 3)^T,\ x_2=(4, 3)^T$，负实例点是 $x_3=(1, 1)^T$，试用感知机学习算法的原始形式求感知机模型 $f(x)=sign(w \cdot x+b)$。这里，$w=(w^{(1)}, w^{(2)})^T,\ x=(x^{(1)}, x^{(2)})^T$。

![]()

**解** 构建最优化问题：

$$
\min_{w, b} L(w, b)=-\sum_{x_i \in M} y_i (w \cdot x_i + b)
$$

按照算法2.1求解 $w,\ b$，$\eta=1$。

（1）取初值 $w_0=0, b_0=0$ 

（2）对 $x_1=(3,\ 3)^T,\ y_1(w_0\cdot\ x_1+b)=0$，未能被正确分类，更新 $w,\ b$ 

$$
w_1 = w_0 + y_1x_1 = (3, 3)^T, b_1 = b_0 + y_1 = 1
$$

得到线性模型

$$
w_1 \cdot x +b_1 = 3x^{(1)} + 3x^{(2)} + 1
$$

（3）对 $x_1,\ x_2$显然，$y_1(w_1 \cdot x_i + b_1) > 0$，被正确分类，不修改 $w,\ b$；对 $x_3=(1,\ 1)^T,\ y_3(w_1 \cdot x_3 + b_1)<0$，被误分类，更新 $w,\ b$。

$$
w_2 = w_1 + y_3 x_3 = (2, 2)^T,\ b_2 = b_1 + y_3 = 0
$$

得到线性模型

$$
w_2 \cdot x + b_2 = 2x^{(1)} + 2x^{(2)}
$$

如此继续下去，直到

$$
w_7 = (1, 1)^T,\ b_7 = -3
$$

$$
w_7 \cdot x + b_7 = x^{(1)} + x^{(2)} - 3
$$

对所有数据点 $y_i(w_7 \cdot x_i + b_7) > 0$，没有误分类点，损失函数达到极小。
分离超平面为： $x^{(1)} + x^{(2)} - 3 = 0$ 
感知机模型为： $f(x) = sign(x^{(1)} + x^{(2)} - 3)$
迭代过程见表 2.1。

`表 2.1 例 2.1 求解的迭代过程` 

| 迭代次数 | 误分类点  | $w$        | $b$  | $w \cdot x + b$           |
|:----:|:-----:|:----------:|:----:|:-------------------------:|
| 0    |       | $0$        | $0$  | $0$                       |
| 1    | $x_1$ | $(3, 3)^T$ | $1$  | $3x^{(1)} + 3x^{(2)} + 1$ |
| 2    | $x_3$ | $(2, 2)^T$ | $0$  | $2x^{(1)} + 2x^{(2)}$     |
| 3    | $x_3$ | $(1, 1)^T$ | $-1$ | $x^{(1)} + x^{(2)} - 1$   |
| 4    | $x_3$ | $(0, 0)^T$ | $-2$ | $-2$                      |
| 5    | $x_1$ | $(3, 3)^T$ | $-1$ | $3x^{(1)} + 3x^{(2)} - 1$ |
| 6    | $x_3$ | $(2, 2)^T$ | $-2$ | $2x^{(1)} + 2x^{(2)} - 2$ |
| 7    | $x_3$ | $(1, 1)^T$ | $-3$ | $x^{(1)} + x^{(2)} - 3$   |
| 8    |       | $(1, 1)^T$ | $-3$ | $x^{(1)} + x^{(2)} - 3$   |

这是在计算中误分类点先后选取 $x_1, x_3, x_3, x_3, x_1, x_3, x_3$ 得到的分离超平面和感知机模型。如果在计算中误分类点依次选取 $x_1, x_3, x_3, x_3, x_2, x_3, x_3, x_3, x_1, x_3, x_3$，那么得到的分离超平面是 $2x^{(1)} + x^{(2)} - 5 = 0$。

可见，**感知机学习算法由于采取不同的初值或选取不同的误分类点，解可以不同**。

## 算法 2.2（感知机学习算法的对偶形式）

**输入**：线性可分的数据集 $T = \{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$，其中 $x_i \in \mathbf{R}^n,\ y_i \in \{-1, +1\}, \ i = 1, 2, ..., N$；学习率 $\eta \ (0< \eta \leqslant 1)$；
**输出**：$\alpha, b$；感知机模型 $f(x) = sign(\sum^N_{j=1} \alpha_j y_j x_j \cdot x + b)$，其中 $\alpha = (\alpha_1, \alpha_2, ..., \alpha_N)^T$ 
（1）$\alpha \leftarrow 0$，$b \leftarrow 0$；
（2）在训练集中选取数据 $(x_i,\ y_i)$；
（3）如果 $y_i(\sum^N_{j=1} \alpha_j y_j x_j \cdot x + b) \leqslant 0$，

$$
\alpha_i \leftarrow \alpha_i + \eta
$$

$$
b \leftarrow b + \eta y_i
$$

（4）转至（2）直到没有误分类数据。

对偶形式中训练样本仅以内积的形式出现。为了方便，可以预先将训练集中样本间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的 Gram 矩阵：

$$
\mathbf{G} = [x_i \cdot x_j]_{N \times N}
$$

### Python 实现

```python

```

### 例 2.2

数据同例 2.1，正样本点是 $x_1=(3, 3)^T,\ x_2=(4, 3)^T$，负样本点是 $x_3=(1, 1)^T$，试用感知机学习算法对偶形式求感知机模型。

**解** 按照算法 2.2，

（1）取 $\alpha_i = 0, \ i = 1, 2, 3, b = 0, \eta = 1$；

（2）计算 Gram 矩阵

$$
\mathbf{G} = \begin{bmatrix}
18 & 21 & 6 \\
21 & 25 & 7 \\
6   & 7   & 2 
\end{bmatrix}
$$

（3）误分类条件

$$
y_i(\sum^N_{j=1} \alpha_j y_j x_j \cdot x_i + b) \leqslant 0
$$

参数更新

$$
\alpha_i \leftarrow \alpha_i + 1,\ b \leftarrow b + y_i
$$

（4）迭代。结果列于表 2.2，最终 $w = 2x_1 - 5x_3 = (1, 1)^T, \ b = -3$；

分离超平面为： $x^{(1)} + x^{(2)} - 3 =0$
感知机模型为： $f(x) = sign(x^{(1)} + x^{(2)} - 3)$

`表 2.2 例 2.2 求解的迭代过程` 

| $k$ | 误分类点  | $\alpha_1$ | $\alpha_2$ | $\alpha_3$ | $b$  |
|:---:|:-----:|:----------:|:----------:|:----------:|:----:|
| 0   |       | $0$        | $0$        | $0$        | $0$  |
| 1   | $x_1$ | $1$        | $0$        | $0$        | $1$  |
| 2   | $x_3$ | $1$        | $0$        | $1$        | $0$  |
| 3   | $x_3$ | $1$        | $0$        | $2$        | $-1$ |
| 4   | $x_3$ | $1$        | $0$        | $3$        | $-2$ |
| 5   | $x_1$ | $2$        | $0$        | $3$        | $-1$ |
| 6   | $x_3$ | $2$        | $0$        | $4$        | $-2$ |
| 7   | $x_3$ | $2$        | $0$        | $5$        | $-3$ |

与原始形式一样，感知机学习算法的对偶形式迭代是收敛的，存在多个解。
