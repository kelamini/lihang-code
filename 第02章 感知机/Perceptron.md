# 感知机

## 算法 2.1（感知机学习算法的原始形式）

**输入**：
**输出**：
（1）
（2）
（3）
（4）

### 感知机算法的 Python 实现

```python

```



### 例 2.1

如图所示的训练数据集，其正实例点是 $x_1=(3, 3)^T$，$x_2=(4, 3)^T$，负实例点是 $x_3=(1, 1)^T$，试用感知机学习算法的原始形式求感知机模型 $f(x)=sign(w \cdot x+b)$。这里，$w=(w^{(1)}, w^{(2)})^T$，$x=(x^{(1)}, x^{(2)})^T$。

![]()

**解** 构建最优化问题：

$$\min_{w, b} L(w, b)=-\sum_{x_i \in M} y_i (w \cdot x_i + b)$$

按照算法2.1求解 $w, b$。$\eta = 1$。

（1）取初值 $w_0 = 0, b_0 = 0$ 

（2）对 $x_1=(3, 3)^T,\ y_1(w_0 \cdot x_1 + b) = 0$，未能被正确分类，更新 $w, b$ 

$$w_1 = w_0 + y_1x_1 = (3, 3)^T, b_1 = b_0 + y_1 = 1$$ 

得到线性模型

$$w_1 \cdot x +b_1 = 3x^{(1)} + 3x^{(2)} + 1$$ 

（3）对 $x_1, x_2$，显然，$y_1(w_1 \cdot x_i + b_1) > 0$，被正确分类，不修改 $w, b$；对 $x_3=(1, 1)^T$，$y_3(w_1 \cdot x_3 + b_1)<0$，被误分类，更新 $w, b$。

$$w_2 = w_1 + y_3 x_3 = (2, 2)^T,\ b_2 = b_1 + y_3 = 0$$ 

得到线性模型

$$w_2 \cdot x + b_2 = 2x^{(1)} + 2x^{(2)}$$ 

如此继续下去，直到

$$w_7 = (1, 1)^T,\ b_7 = -3$$ 

$$w_7 \cdot x + b_7 = x^{(1)} + x^{(2)} - 3$$ 

对所有数据点 $y_i(w_7 \cdot x_i + b_7) > 0$，没有误分类点，损失函数达到极小。
分离超平面为：$x^{(1)} + x^{(2)} - 3 = 0$ 
感知机模型为：$f(x) = sign(x^{(1)} + x^{(2)} - 3)$ 
迭代过程见表2.1。

表2.1 例2.1求解的迭代过程

| 迭代次数 | 误分类点 |    $w$     | $b$  |      $w \cdot x + b$      |
| :------: | :------: | :--------: | :--: | :-----------------------: |
|    0     |          |    $0$     | $0$  |            $0$            |
|    1     |  $x_1$   | $(3, 3)^T$ | $1$  | $3x^{(1)} + 3x^{(2)} + 1$ |
|    2     |  $x_3$   | $(2, 2)^T$ | $0$  |   $2x^{(1)} + 2x^{(2)}$   |
|    3     |  $x_3$   | $(1, 1)^T$ | $-1$ |  $x^{(1)} + x^{(2)} - 1$  |
|    4     |  $x_3$   | $(0, 0)^T$ | $-2$ |           $-2$            |
|    5     |  $x_1$   | $(3, 3)^T$ | $-1$ | $3x^{(1)} + 3x^{(2)} - 1$ |
|    6     |  $x_3$   | $(2, 2)^T$ | $-2$ | $2x^{(1)} + 2x^{(2)} - 2$ |
|    7     |  $x_3$   | $(1, 1)^T$ | $-3$ |  $x^{(1)} + x^{(2)} - 3$  |
|    8     |          | $(1, 1)^T$ | $-3$ |  $x^{(1)} + x^{(2)} - 3$  |

这是在计算中误分类点先后选取 $x_1, x_3, x_3, x_3, x_1, x_3, x_3$ 得到的分离超平面和感知机模型。如果在计算中误分类点依次选取 $x_1, x_3, x_3, x_3, x_2, x_3, x_3, x_3, x_1, x_3, x_3$，那么得到的分离超平面是 $2x^{(1)} + x^{(2)} - 5 = 0$。

可见，**感知机学习算法由于采取不同的初值或选取不同的误分类点，解可以不同**。



## 算法 2.2（感知机学习算法的对偶形式）

输入：
输出：
（1）
（2）
（3）
（4）

### 例 2.2

