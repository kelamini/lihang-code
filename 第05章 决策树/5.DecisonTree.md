# 决策树

## 算法 5.1（信息增益的算法）

**输入**：训练数据集 $D$ 和特征 $A$；

**输出**：特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D, A)$。

（1）计算数据集 $D$ 的经验熵 $H(D)$ 

$$
H(D) = - \sum^K_{k=1} \frac{|C_k|}{|D|}log_2 \frac{|C_k|}{|D|}
$$

（2）计算特征 $A$ 对数据集 $D$ 的经验条件熵 $D(D|A)$ 

$$
H(D|A) = \sum^n_{i=1}\frac{|D_i|}{|D|}H(D_i) = -\sum^n_{i=1}\frac{|D_i|}{|D|}\sum^K_{k=1}\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}
$$

（3）计算信息增益

$$
g(D, A) = H(D)-H(D|A)
$$

### 例 5.2

对表 5.1 所给的训练数据集 $D$，根据信息增益准则选择最优特征。

`表 5.1 贷款申请样本数据表`

| ID  | 年龄  | 有工作 | 有自己的房子 | 信贷情况 | 类别  |
|:---:|:---:|:---:|:------:|:----:|:---:|
| 1   | 青年  | 否   | 否      | 一般   | 否   |
| 2   | 青年  | 否   | 否      | 好    | 否   |
| 3   | 青年  | 是   | 否      | 好    | 是   |
| 4   | 青年  | 是   | 是      | 一般   | 是   |
| 5   | 青年  | 否   | 否      | 一般   | 否   |
| 6   | 中年  | 否   | 否      | 一般   | 否   |
| 7   | 中年  | 否   | 否      | 好    | 否   |
| 8   | 中年  | 是   | 是      | 好    | 是   |
| 9   | 中年  | 否   | 是      | 非常好  | 是   |
| 10  | 中年  | 否   | 是      | 非常好  | 是   |
| 11  | 老年  | 否   | 是      | 非常好  | 是   |
| 12  | 老年  | 否   | 是      | 好    | 是   |
| 13  | 老年  | 是   | 否      | 好    | 是   |
| 14  | 老年  | 是   | 否      | 非常好  | 是   |
| 15  | 老年  | 否   | 否      | 一般   | 否   |

**解** 首先计算经验熵 $H(D)$ 

$$
H(D) = -\frac{9}{15}log_2\frac{9}{15}-\frac{6}{15}log_2\frac{6}{15} = 0.971
$$

然后计算各个特征对数据集 $D$ 的信息增益。分别以 $A_1$， $A_2$， $A_3$， $A_4$ 表示年龄、有工作、有自己的房子和信贷情况 4 个特征，则

（1）

$$
g(D, A_1) = H(D)-[\frac{5}{15}H(D_1)+\frac{5}{15}H(D_2)+\frac{5}{15}H(D_3)]
$$

$$
= 0.971-[\frac{5}{15}(-\frac{2}{5}log_2\frac{2}{5}-\frac{3}{5}log_2\frac{3}{5})+\frac{5}{15}(-\frac{3}{5}log_2\frac{3}{5}-\frac{2}{5}log_2\frac{2}{5})+\frac{5}{15}(-\frac{4}{5}log_2\frac{4}{5}-\frac{1}{5}log_2\frac{1}{5})]
$$

$$
=0.971-0.888=0.083
$$

这里 $D_1$， $D_2$， $D_3$ 分别是 $D$ 中 $A_1$（年龄）取值为青年、中年和老年的样本子集。类似地，

（2）

$$
g(D, A_2) = H(D)-[\frac{5}{15}H(D_1)+\frac{10}{15}H(D_2)]
$$

$$
= 0.971-[\frac{5}{15}\times0+\frac{10}{15}(-\frac{4}{10}log_2\frac{4}{10}-\frac{6}{10}log_2\frac{6}{10})] = 0.324
$$

（3）

$$
g(D, A_3) = 0.971-[\frac{6}{15}\times0+\frac{9}{15}(-\frac{3}{9}log_2\frac{3}{9}-\frac{6}{9}log_2\frac{6}{9})] = 0.971-0.551 = 0.420
$$

（4）

$$
g(D, A_4) = 0.971-0.608 = 0.363
$$

最后，比较各特征的信息增益值，由于特征 $A_3$ （有自己的房子）的信息增益值最大，所以选择特征 $A_3$ 作为最优特征。

## 算法 5.2（ID3 算法）

输入：训练数据集 $D$，特征集 $A$，阈值 $\epsilon$；

输出：决策树 $T$。

（1）若 $D$ 中所有实例属于同一类 $C_k$，则 $T$ 为单结点树，并将类 $C_k$ 作为该结点的类标记，返回 $T$；

（2）若 $A=\varnothing$，则 $T$ 为单节点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $T$；

（3）否则，按算法 5.1 计算 $A$ 中各特征对 $D$ 的信息增益，选择信息增益最大的特征 $A_g$；

（4）如果 $A_g$ 的信息增益小于阈值 $\epsilon$，则置 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $T$；

（5）否则，对 $A_g$ 的每一可能值 $a_i$，依 $A_g = a_i$ 将 $D$ 分割为若干非空子集 $D_i$，将 $D_i$ 中实例数最大的类作为标记，构建子结点，由结点及子结点构成树 $T$，返回 $T$；

（6）对第 $i$ 个子结点，以 $D_i$ 为训练集，以 $A-\{A_g\}$ 为特征集，递归的调用步（1）~（5），得到子树 $T_i$，返回 $T_i$。

### 例 5.3

对表 5.1 的训练数据集，利用 ID3 算法建立决策树。